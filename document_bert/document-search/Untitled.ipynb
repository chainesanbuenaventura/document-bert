{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from clean import TextCleaner\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertConfig, BertModel, PreTrainedModel\n",
    "from encode import BERTEncoder\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import gc \n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from patent_utils import *\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSimilarityTrainer(object):\n",
    "    def __init__(self, patent_documents: list, tsd_documents: list, labels: list, threshold: float = 0.5, patience: int = 3):\n",
    "        self.patent_documents = patent_documents\n",
    "        self.tsd_documents = tsd_documents\n",
    "        self.labels = labels\n",
    "        self.bertEncoder = BERTEncoder(self.patent_documents, self.tsd_documents, self.labels, PatentCleaner())\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        self.avg_train_losses = []\n",
    "        self.avg_valid_losses = [] \n",
    "        self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "        self.loss_function = torch.nn.BCELoss()\n",
    "        self.max_input_length = 512\n",
    "        self.config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        self.config.bert_batch_size = 20\n",
    "        self.config.num_labels = 1\n",
    "        self.model = self.get_model()\n",
    "        self.start_time = get_datetime()\n",
    "        self.patience = patience\n",
    "        self.early_stopping = EarlyStopping(self.model, self.patience, self.start_time, verbose=True)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                     weight_decay=0,\n",
    "                                     lr=5e-5)\n",
    "        self.threshold = threshold\n",
    "        self.batch_size = 8\n",
    "\n",
    "    def validate(self):\n",
    "        patent_representations, tsd_representations, correct_output, tsd_dict = self.bertEncoder.tokenize_test_data()\n",
    "        gc.collect()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        total_correct, total_predictions = 0, 0\n",
    "\n",
    "        for i in range(0, patent_representations.shape[0], self.batch_size):\n",
    "\n",
    "            batch_patent_tensors = patent_representations[i:i + self.batch_size].to(device=self.device)\n",
    "            \n",
    "            batch_tsd_tensors = tsd_representations[i:i + self.batch_size].to(device=self.device)\n",
    "\n",
    "            batch_predictions = self.model(batch_patent_tensors,\n",
    "                                      batch_tsd_tensors, \n",
    "                                      device=self.device\n",
    "                                     )\n",
    "\n",
    "            batch_correct_output = correct_output[i:i + self.batch_size].to(device=self.device)\n",
    "            \n",
    "            loss = self.loss_function(batch_predictions, batch_correct_output.view(batch_predictions.shape))\n",
    "            self.valid_losses.append(loss.item())\n",
    "        \n",
    "            valid_loss = np.average(self.valid_losses)\n",
    "            self.avg_valid_losses.append(valid_loss)\n",
    "\n",
    "            if self.threshold == 0.5:\n",
    "                num_correct = (batch_predictions.T[0].round() == batch_correct_output).sum().item()\n",
    "            else:\n",
    "                num_correct = (round_custom(batch_predictions.T[0], self.threshold) == batch_correct_output).sum().item()\n",
    "            num_predictions = len(batch_predictions)\n",
    "            total_correct += num_correct\n",
    "            total_predictions += num_predictions\n",
    "        \n",
    "        valid_acc = total_correct/total_predictions * 100\n",
    "#         print(f\"\\tTest accuracy={total_correct/total_predictions * 100:.2f}\")\n",
    "        \n",
    "        self.valid_losses = []\n",
    "        \n",
    "        return valid_acc\n",
    "        \n",
    "    def train(self):\n",
    "        patent_representations, tsd_representations, correct_output, tsd_dict = self.bertEncoder.tokenize_train_data()\n",
    "        self.tsd_dict = tsd_dict\n",
    "        gc.collect()\n",
    "        total_correct, total_predictions = 0, 0\n",
    "        early_stopping_counter = 0\n",
    "        best_train_acc = 0.00\n",
    "\n",
    "        for epoch in range(100):\n",
    "            self.model.train()\n",
    "            permutation = torch.randperm(patent_representations.shape[0])\n",
    "            patent_representations = patent_representations[permutation]\n",
    "            tsd_representations = tsd_representations[permutation]\n",
    "            correct_output = correct_output[permutation]\n",
    "\n",
    "            epoch = epoch\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for i in tqdm(range(0, patent_representations.shape[0], self.batch_size), bar_format='{desc:<5.5}{percentage:3.0f}%|{bar:10}{r_bar}'):\n",
    "                batch_patent_tensors = patent_representations[i:i + self.batch_size].to(device=self.device)\n",
    "                batch_tsd_tensors = tsd_representations[i:i + self.batch_size].to(device=self.device)\n",
    "\n",
    "                batch_predictions = self.model(batch_patent_tensors,\n",
    "                                          batch_tsd_tensors, \n",
    "                                          device=self.device\n",
    "                                         )\n",
    "\n",
    "                batch_correct_output = correct_output[i:i + self.batch_size].to(device=self.device)\n",
    "\n",
    "                loss = self.loss_function(batch_predictions, batch_correct_output.view(batch_predictions.shape))\n",
    "                self.train_losses.append(loss.item())\n",
    "                \n",
    "                train_loss = np.average(self.train_losses)\n",
    "                self.avg_train_losses.append(train_loss)\n",
    "                \n",
    "                epoch_loss += float(loss.item())\n",
    "                #self.log.info(batch_predictions)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.threshold == 0.5:\n",
    "                    num_correct = (batch_predictions.T[0].round() == batch_correct_output).sum().item()\n",
    "                else:\n",
    "                    num_correct = (round_custom(batch_predictions.T[0], self.threshold) == batch_correct_output).sum().item()\n",
    "                num_predictions = len(batch_predictions)\n",
    "                total_correct += num_correct\n",
    "                total_predictions += num_predictions\n",
    "\n",
    "            self.train_losses = []\n",
    "            \n",
    "            train_acc = total_correct/total_predictions * 100\n",
    "#             if train_acc < best_train_acc:\n",
    "#                 early_stopping_counter += 1\n",
    "#             else:\n",
    "#                 early_stopping_counter = 0\n",
    "            epoch_loss /= int(patent_representations.shape[0] / self.batch_size)  # divide by number of batches per epoch\n",
    "            valid_acc = self.validate()   \n",
    "            print(f\"Train: Epoch {epoch}, Training Loss={epoch_loss:4f}, Train accuracy={total_correct/total_predictions * 100:.2f}%, Test accuracy={valid_acc:.2f}%\")\n",
    "            self.early_stopping.record(valid_acc, epoch)\n",
    "              \n",
    "#             if early_stopping_counter == 3:\n",
    "#                 break; \n",
    "    \n",
    "            if self.early_stopping():\n",
    "                print(\"Early stopping\")\n",
    "                break;\n",
    "            \n",
    "    def get_model(self):\n",
    "        model = DocumentBert.from_pretrained('bert-base-uncased', config=self.config)\n",
    "        model.freeze_bert_encoder()\n",
    "        model.unfreeze_bert_encoder_last_layers()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = torch.nn.DataParallel(self.bert_doc_classification)\n",
    "        model.to(device=self.device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentBert(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, bert_model_config: BertConfig):\n",
    "        super(DocumentBert, self).__init__(bert_model_config)\n",
    "        self.bert_patent = BertModel(bert_model_config)\n",
    "        self.bert_tsd = BertModel(bert_model_config)\n",
    "        \n",
    "        for param in self.bert_patent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.bert_tsd.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.bert_batch_size = self.bert_patent.config.bert_batch_size  \n",
    "        self.dropout_patent = torch.nn.Dropout(p=bert_model_config.hidden_dropout_prob)\n",
    "        self.dropout_tsd = torch.nn.Dropout(p=bert_model_config.hidden_dropout_prob)\n",
    "        \n",
    "        self.lstm_patent = torch.nn.LSTM(bert_model_config.hidden_size,bert_model_config.hidden_size)\n",
    "        self.lstm_tsd = torch.nn.LSTM(bert_model_config.hidden_size,bert_model_config.hidden_size)\n",
    "        \n",
    "        self.output = torch.nn.Linear(bert_model_config.hidden_size*2, out_features=1)\n",
    "\n",
    "    def forward(self, patent_batch: torch.Tensor, tsd_batch: torch.Tensor, device='cuda'):\n",
    "\n",
    "        #patent\n",
    "        bert_output_patent = torch.zeros(size=(patent_batch.shape[0],\n",
    "                                              min(patent_batch.shape[1],self.bert_batch_size),\n",
    "                                              self.bert_patent.config.hidden_size), dtype=torch.float, device=device)\n",
    "        for doc_id in range(patent_batch.shape[0]):\n",
    "            bert_output_patent[doc_id][:self.bert_batch_size] = self.dropout_patent(self.bert_patent(patent_batch[doc_id][:self.bert_batch_size,0],\n",
    "                                            token_type_ids=patent_batch[doc_id][:self.bert_batch_size,1],\n",
    "                                            attention_mask=patent_batch[doc_id][:self.bert_batch_size,2])[1])\n",
    "        output_patent, (_, _) = self.lstm_patent(bert_output_patent.permute(1,0,2))\n",
    "        last_layer_patent = output_patent[-1]\n",
    "        \n",
    "        #tsd\n",
    "\n",
    "        bert_output_tsd = torch.zeros(size=(tsd_batch.shape[0],\n",
    "                                              min(tsd_batch.shape[1],self.bert_batch_size),\n",
    "                                              self.bert_tsd.config.hidden_size), dtype=torch.float, device=device)\n",
    "        for doc_id in range(tsd_batch.shape[0]):\n",
    "            bert_output_tsd[doc_id][:self.bert_batch_size] = self.dropout_tsd(self.bert_tsd(tsd_batch[doc_id][:self.bert_batch_size,0],\n",
    "                                            token_type_ids=tsd_batch[doc_id][:self.bert_batch_size,1],\n",
    "                                            attention_mask=tsd_batch[doc_id][:self.bert_batch_size,2])[1])\n",
    "        output_tsd, (_, _) = self.lstm_tsd(bert_output_tsd.permute(1,0,2))\n",
    "        last_layer_tsd = output_tsd[-1]\n",
    "       \n",
    "        x = torch.cat([last_layer_patent, last_layer_tsd], dim=1)\n",
    "        prediction = torch.nn.functional.sigmoid(self.output(x))\n",
    "        \n",
    "        assert prediction.shape[0] == patent_batch.shape[0]\n",
    "        return prediction\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert_patent.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bert_tsd.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert_patent.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.bert_tsd.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_bert_encoder_last_layers(self):\n",
    "        for name, param in self.bert_patent.named_parameters():\n",
    "            if \"encoder.layer.11\" in name or \"pooler\" in name:\n",
    "                param.requires_grad = True\n",
    "        for name, param in self.bert_tsd.named_parameters():\n",
    "            if \"encoder.layer.11\" in name or \"pooler\" in name:\n",
    "                param.requires_grad = True\n",
    "    def unfreeze_bert_encoder_pooler_layer(self):\n",
    "        for name, param in self.bert_patent.named_parameters():\n",
    "            if \"pooler\" in name:\n",
    "                param.requires_grad = True\n",
    "        for name, param in self.bert_tsd.named_parameters():\n",
    "            if \"pooler\" in name:\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, patent_claim):\n",
    "    patent_representations, output = bertEncoder.encode_patents([patent_claim] * len(self.tsd_dict))\n",
    "\n",
    "    patent_representations = patent_representations.to('cpu')\n",
    "    tsd_representations = tsd_representations.to('cpu')\n",
    "    model.to('cpu')\n",
    "\n",
    "    similarity_scores = []\n",
    "    batch_size = 4\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i in range(0, patent_representations.shape[0], batch_size):\n",
    "\n",
    "        batch_patent_tensors = patent_representations[i:i + batch_size].to(device='cpu')\n",
    "        batch_tsd_tensors = tsd_representations[i:i + batch_size].to(device='cpu')\n",
    "\n",
    "        batch_predictions = model(batch_patent_tensors,\n",
    "                          batch_tsd_tensors, \n",
    "                          device='cpu'\n",
    "                         )\n",
    "        similarity_scores.extend(batch_predictions)\n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
